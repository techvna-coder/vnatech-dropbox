# streamlit_app.py
# -*- coding: utf-8 -*-
import os
import pickle
from io import BytesIO
from typing import List, Dict, Any, Tuple
from collections import defaultdict

import streamlit as st
import numpy as np
import pandas as pd

from datetime import datetime, timezone

# FAISS
try:
    import faiss
except Exception:
    st.error("FAISS is required. Please add 'faiss-cpu>=1.7.4' to requirements.txt")
    st.stop()

# OpenAI
try:
    from openai import OpenAI
except Exception:
    st.error("OpenAI SDK is required. Please add 'openai>=1.12.0' to requirements.txt")
    st.stop()

# Bcrypt (custom auth)
try:
    import bcrypt
except Exception:
    st.error("Please add 'bcrypt>=4.0.1' to requirements.txt")
    st.stop()

# Local modules (Dropbox Edition)
try:
    from dropbox_utils import (
        authenticate_dropbox,
        list_files_in_folder,
        download_file,
        format_file_size,
        download_embeddings_from_dropbox,
        upload_embeddings_to_dropbox,
    )
except Exception as e:
    st.error("Failed to import dropbox_utils: %s" % e)
    st.stop()

try:
    from document_processors import (
        process_pdf,
        process_pptx,
        chunk_text_smart,
        get_embeddings,
        count_tokens,
    )
except Exception as e:
    st.error("Failed to import document_processors: %s" % e)
    st.stop()

# =========================
# App Constants & Settings
# =========================
EMBEDDINGS_FILE = "embeddings_meta.pkl"
FAISS_INDEX_FILE = "faiss_index.bin"
TOP_K = 10

st.set_page_config(page_title="VNA Tech (Dropbox)", layout="wide")

# =========================
# Authentication (gi·ªØ nguy√™n)
# =========================

def _load_credentials_from_secrets() -> Dict[str, Dict[str, str]]:
    if "auth" not in st.secrets:
        raise RuntimeError("Missing [auth] in secrets.")
    users = st.secrets["auth"].get("users", {})
    creds = {}
    for _, u in users.items():
        uname = u.get("username")
        pwd = u.get("password")
        name = u.get("name", uname)
        if uname and pwd:
            creds[uname] = {"name": name, "password": pwd}
    if not creds:
        raise RuntimeError("No valid users under [auth.users].")
    return creds


def _verify_password(plain: str, hashed: str) -> bool:
    try:
        return bcrypt.checkpw(plain.encode("utf-8"), hashed.encode("utf-8"))
    except Exception:
        return False


def login_gate() -> Tuple[bool, str, str]:
    try:
        creds = _load_credentials_from_secrets()
    except Exception as e:
        st.error(str(e))
        st.stop()

    if "auth_user" in st.session_state and st.session_state.get("auth_ok"):
        u = st.session_state["auth_user"]
        display_name = st.session_state.get("auth_name", u)
        return True, u, display_name

    with st.form("login_form", clear_on_submit=False):
        st.subheader("ƒêƒÉng nh·∫≠p ƒë·ªÉ truy c·∫≠p VNA Techinsight Hub (Dropbox)")
        username = st.text_input("Username")
        password = st.text_input("Password", type="password")
        submitted = st.form_submit_button("Login")

    if submitted:
        if username in creds and _verify_password(password, creds[username]["password"]):
            st.session_state["auth_ok"] = True
            st.session_state["auth_user"] = username
            st.session_state["auth_name"] = creds[username]["name"]
            st.success("ƒêƒÉng nh·∫≠p th√†nh c√¥ng.")
            st.rerun()
        else:
            st.error("Sai username ho·∫∑c password.")

    return False, "", ""


def logout_button():
    if st.session_state.get("auth_ok"):
        if st.sidebar.button("Sign out"):
            for k in ["auth_ok", "auth_user", "auth_name"]:
                if k in st.session_state:
                    del st.session_state[k]
            st.success("ƒê√£ ƒëƒÉng xu·∫•t.")
            st.rerun()

# =========================
# Dropbox Helpers
# =========================
@st.cache_resource(show_spinner=False)
def _dropbox_client():
    return authenticate_dropbox()


def _list_dropbox_files() -> List[Dict[str, Any]]:
    folder_path = st.secrets.get("DROPBOX_FOLDER_PATH") or os.getenv("DROPBOX_FOLDER_PATH", "/Apps/VNATechInsight")
    if not folder_path:
        st.error("DROPBOX_FOLDER_PATH is missing in secrets or env.")
        st.stop()
    dbx = _dropbox_client()
    files = list_files_in_folder(dbx, folder_path)
    filtered = []
    for f in files:
        name = f.get("name", "")
        if name.lower().endswith(".pdf") or name.lower().endswith(".pptx"):
            # Attach a path for downloader
            f["path"] = f.get("path_display") or f.get("path_lower") or name
            filtered.append(f)
    return filtered

# =========================
# Embeddings Store & FAISS
# =========================

def _try_load_local_index():
    if os.path.exists(EMBEDDINGS_FILE) and os.path.exists(FAISS_INDEX_FILE):
        try:
            with open(EMBEDDINGS_FILE, "rb") as f:
                meta = pickle.load(f)
            index = faiss.read_index(FAISS_INDEX_FILE)
            return index, meta
        except Exception:
            return None, None
    return None, None


def _load_or_pull_cache_from_dropbox() -> Tuple[Any, List[Dict[str, Any]]]:
    idx, meta = _try_load_local_index()
    if idx is not None and meta is not None:
        return idx, meta
    dbx = _dropbox_client()
    folder_path = st.secrets.get("DROPBOX_FOLDER_PATH") or os.getenv("DROPBOX_FOLDER_PATH", "/Apps/VNATechInsight")
    paths = download_embeddings_from_dropbox(dbx, folder_path, EMBEDDINGS_FILE, FAISS_INDEX_FILE)
    if paths.get("embeddings_path") and paths.get("faiss_path"):
        try:
            with open(EMBEDDINGS_FILE, "rb") as f:
                meta = pickle.load(f)
            idx = faiss.read_index(FAISS_INDEX_FILE)
            return idx, meta
        except Exception:
            pass
    return None, None


def _get_processed_file_ids(meta: List[Dict[str, Any]]) -> set:
    if not meta:
        return set()
    return {item.get("file_id") for item in meta if item.get("file_id")}


def _build_or_load_index(process_all: bool = False) -> Tuple[Any, List[Dict[str, Any]]]:
    dbx = _dropbox_client()
    files = _list_dropbox_files()

    existing_index = None
    existing_meta = []
    processed_ids = set()

    if not process_all:
        existing_index, existing_meta = _load_or_pull_cache_from_dropbox()
        if existing_index is not None and existing_meta is not None:
            processed_ids = _get_processed_file_ids(existing_meta)
            st.info(f"üì¶ ƒê√£ load {len(existing_meta)} chunks t·ª´ {len(processed_ids)} files c√≥ s·∫µn")

    # Dropbox file id may be stable; use (path) as identity
    new_files = [f for f in files if (f.get("id") or f.get("path")) not in processed_ids]

    if not new_files and existing_index is not None:
        st.success("‚úÖ Kh√¥ng c√≥ file m·ªõi. S·ª≠ d·ª•ng index hi·ªán t·∫°i.")
        return existing_index, existing_meta

    if new_files:
        st.info(f"üìÑ Ph√°t hi·ªán {len(new_files)} file m·ªõi c·∫ßn x·ª≠ l√Ω")

    new_vectors = []
    new_meta: List[Dict[str, Any]] = []

    progress = st.progress(0.0, text="Processing new documents...")
    n = max(len(new_files), 1)

    for i, f in enumerate(new_files, start=1):
        file_id = f.get("id") or f.get("path")
        file_name = f.get("name")
        file_mtime = f.get("modifiedTime")
        file_path = f.get("path")
        progress.progress(i / n, text="Processing %s (%d/%d)" % (file_name, i, len(new_files)))

        try:
            content: BytesIO = download_file(dbx, file_path)
        except Exception as e:
            st.warning("Failed to download '%s': %s" % (file_name, e))
            continue

        try:
            if file_name.lower().endswith(".pdf"):
                text, meta = process_pdf(content)
            elif file_name.lower().endswith(".pptx"):
                text, meta = process_pptx(content)
            else:
                continue
        except Exception as e:
            st.warning("Failed to parse '%s': %s" % (file_name, e))
            continue

        chunks = chunk_text_smart(text, meta, chunk_size=1000, chunk_overlap=200)
        texts = [c["text"] for c in chunks]

        try:
            vecs = get_embeddings(texts, batch_size=100)
        except Exception as e:
            st.error("Embedding failed for %s: %s" % (file_name, e))
            continue

        for j, c in enumerate(chunks):
            new_vectors.append(vecs[j])
            row = {"file_id": file_id, "file_name": file_name, "modified_time": file_mtime}
            row.update(c)
            new_meta.append(row)

    progress.progress(1.0, text="Ho√†n th√†nh x·ª≠ l√Ω file m·ªõi")

    if not new_vectors and not existing_meta:
        st.error("No embeddings were created. Please check your Dropbox folder and parsers.")
        st.stop()

    if new_vectors:
        new_mat = np.array(new_vectors, dtype="float32")
        faiss.normalize_L2(new_mat)

        if existing_index is not None and existing_meta:
            existing_index.add(new_mat)
            combined_meta = existing_meta + new_meta
            st.success(f"‚úÖ ƒê√£ th√™m {len(new_vectors)} chunks m·ªõi v√†o index (t·ªïng: {len(combined_meta)} chunks)")
            index = existing_index
            all_meta = combined_meta
        else:
            index = faiss.IndexFlatIP(new_mat.shape[1])
            index.add(new_mat)
            all_meta = new_meta
            st.success(f"‚úÖ ƒê√£ t·∫°o index m·ªõi v·ªõi {len(new_meta)} chunks")
    else:
        index = existing_index
        all_meta = existing_meta

    # Save local caches
    with open(EMBEDDINGS_FILE, "wb") as f:
        pickle.dump(all_meta, f)
    faiss.write_index(index, FAISS_INDEX_FILE)

    # (Optional) Upload caches back to Dropbox for portability
    try:
        folder_path = st.secrets.get("DROPBOX_FOLDER_PATH") or os.getenv("DROPBOX_FOLDER_PATH", "/Apps/VNATechInsight")
        upload_embeddings_to_dropbox(_dropbox_client(), folder_path, EMBEDDINGS_FILE, FAISS_INDEX_FILE)
    except Exception:
        pass

    return index, all_meta

# =========================
# Retrieval & Reranking (gi·ªØ nguy√™n logic)
# =========================

def _embed_query(client: OpenAI, query: str) -> np.ndarray:
    resp = client.embeddings.create(model="text-embedding-3-small", input=[query])
    v = np.array(resp.data[0].embedding, dtype="float32")
    v = v / np.linalg.norm(v)
    return v


def _keyword_score(query: str, text: str, key_terms: List[str]) -> float:
    query_lower = query.lower()
    text_lower = text.lower()
    score = 0.0
    query_words = set(query_lower.split())
    text_words = set(text_lower.split())
    common_words = query_words & text_words
    score += len(common_words) * 0.1
    for term in key_terms:
        if term.lower() in query_lower and term.lower() in text_lower:
            score += 0.3
    from itertools import tee
    def bigrams(tokens):
        a, b = tee(tokens)
        next(b, None)
        return set(zip(a, b))
    q_tokens = query_lower.split()
    t_tokens = text_lower.split()
    common_bi = bigrams(q_tokens) & bigrams(t_tokens)
    score += len(common_bi) * 0.2
    return min(score, 1.0)


def _rerank_results(query: str, results: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
    for r in results:
        semantic_score = r["similarity"]
        all_terms = r.get("local_key_terms", [])
        keyword_score = _keyword_score(query, r["text"], all_terms)
        content_type = r.get("content_type", "general")
        type_bonus = 0.0
        if content_type in ["procedure", "specification"]:
            type_bonus = 0.1
        elif content_type == "safety_note":
            type_bonus = 0.15
        if r.get("is_complete_section", False):
            type_bonus += 0.05
        if r.get("has_tables", False):
            type_bonus += 0.05
        if r.get("has_lists", False):
            type_bonus += 0.03
        combined = (semantic_score * 0.65 + keyword_score * 0.25 + type_bonus * 0.10)
        r["rerank_score"] = combined
        r["keyword_score"] = keyword_score
    reranked = sorted(results, key=lambda x: x["rerank_score"], reverse=True)
    diverse_results = []
    file_counts = defaultdict(int)
    max_per_file = max(2, top_k // 3)
    for r in reranked:
        file_name = r["file_name"]
        if file_counts[file_name] < max_per_file or len(diverse_results) < top_k:
            diverse_results.append(r)
            file_counts[file_name] += 1
            if len(diverse_results) >= top_k:
                break
    if len(diverse_results) < top_k:
        for r in reranked:
            if r not in diverse_results:
                diverse_results.append(r)
                if len(diverse_results) >= top_k:
                    break
    return diverse_results[:top_k]


def _search(index, meta: List[Dict[str, Any]], qvec: np.ndarray, query: str, topk: int = TOP_K):
    D, I = index.search(qvec.reshape(1, -1), topk * 2)
    candidates = []
    for score, idx in zip(D[0].tolist(), I[0].tolist()):
        if idx < 0 or idx >= len(meta):
            continue
        item = meta[idx].copy()
        item["similarity"] = float(score)
        candidates.append(item)
    final_results = _rerank_results(query, candidates, top_k=topk)
    return final_results


def _format_context(chunks: List[Dict[str, Any]]) -> str:
    blocks = []
    for i, c in enumerate(chunks, 1):
        file_name = c["file_name"]
        section = f"{str(c.get('section_type', '?')).title()} {c.get('section_number', '?')}"
        title = c.get("section_title", "")
        title_str = f" - {title}" if title else ""
        content_type = c.get("content_type", "general")
        header = f"[{i}] {file_name} | {section}{title_str}\n"
        header += f"Type: {content_type} | Relevance: {c.get('rerank_score', 0):.3f}\n"
        if c.get("has_tables"): header += "‚ö†Ô∏è Contains table data\n"
        if c.get("has_lists"): header += "üìã Contains structured list\n"
        text = c["text"]
        blocks.append(header + "---\n" + text)
    return "\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n".join(blocks)


def _ask_llm(client: OpenAI, question: str, chunks: List[Dict[str, Any]]) -> str:
    context = _format_context(chunks)
    system = """B·∫°n l√† tr·ª£ l√Ω k·ªπ thu·∫≠t chuy√™n nghi·ªáp c·ªßa Vietnam Airlines.

NHI·ªÜM V·ª§:
1. ƒê·ªçc k·ªπ v√† ph√¢n t√≠ch t·∫•t c·∫£ ngu·ªìn tham chi·∫øu ƒë∆∞·ª£c cung c·∫•p
2. Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a HO√ÄN TO√ÄN tr√™n th√¥ng tin trong ngu·ªìn tham chi·∫øu
3. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ph·∫ßn n√†o thi·∫øu
4. Tr√≠ch d·∫´n r√µ r√†ng ngu·ªìn b·∫±ng c√°ch ghi [s·ªë] t∆∞∆°ng ·ª©ng v·ªõi ngu·ªìn

C√ÅCH TR·∫¢ L·ªúI:
- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát, ch√≠nh x√°c v√† chuy√™n nghi·ªáp
- C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi r√µ r√†ng (d√πng ƒë·∫ßu d√≤ng n·∫øu c·∫ßn)
- V·ªõi th√¥ng tin k·ªπ thu·∫≠t: ghi ƒë·∫ßy ƒë·ªß s·ªë li·ªáu, ƒë∆°n v·ªã, ƒëi·ªÅu ki·ªán
- V·ªõi quy tr√¨nh: li·ªát k√™ c√°c b∆∞·ªõc theo th·ª© t·ª±
- Lu√¥n tr√≠ch d·∫´n ngu·ªìn b·∫±ng [1], [2], [3]... sau m·ªói th√¥ng tin

QUAN TR·ªåNG:
- KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong ngu·ªìn
- KH√îNG t√≥m t·∫Øt qu√° ng·∫Øn g·ªçn n·∫øu c√¢u h·ªèi y√™u c·∫ßu chi ti·∫øt
- ∆Øu ti√™n th√¥ng tin t·ª´ ngu·ªìn c√≥ "Relevance" cao h∆°n"""

    user_msg = f"""C√¢u h·ªèi: {question}

NGU·ªíN THAM CHI·∫æU:
{context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c ngu·ªìn tr√™n. Nh·ªõ tr√≠ch d·∫´n ngu·ªìn b·∫±ng [s·ªë]."""

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user_msg},
    ]
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.1,
            max_tokens=2000,
        )
        return resp.choices[0].message.content
    except Exception as e:
        st.error(f"LLM API error: {e}")
        return "Xin l·ªói, ƒë√£ c√≥ l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i."

# =========================
# UI
# =========================

def sidebar_panel(index, meta):
    st.sidebar.header("VNA Techinsight ‚Äì Dropbox")

    processed_ids = _get_processed_file_ids(meta)
    with st.sidebar.expander("üìä Th·ªëng k√™", expanded=True):
        st.metric("S·ªë files ƒë√£ x·ª≠ l√Ω", len(processed_ids))
        st.metric("T·ªïng s·ªë chunks", len(meta) if meta else 0)
        if meta:
            content_types = [m.get("content_type", "general") for m in meta]
            type_counts = pd.Series(content_types).value_counts()
            st.caption("**Content Types:**")
            for ctype, count in type_counts.items():
                st.caption(f"  ‚Ä¢ {ctype}: {count}")
        st.caption("Cache ƒë∆∞·ª£c l∆∞u **local** v√† ƒë·ªìng b·ªô qua **Dropbox**.")

    st.sidebar.divider()

    with st.sidebar.expander("üîß Qu·∫£n l√Ω Index", expanded=False):
        st.write("**Embeddings**: `%s`" % EMBEDDINGS_FILE)
        st.write("**FAISS index**: `%s`" % FAISS_INDEX_FILE)
        st.divider()
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ C·∫≠p nh·∫≠t (ch·ªâ file m·ªõi)", use_container_width=True):
                st.session_state["force_rebuild"] = False
                st.rerun()
        with col2:
            if st.button("üî® Rebuild to√†n b·ªô", type="secondary", use_container_width=True):
                st.session_state["force_rebuild"] = True
                st.rerun()
        if st.button("üóëÔ∏è Xo√° cache (local)", type="secondary", use_container_width=True):
            try:
                if os.path.exists(EMBEDDINGS_FILE):
                    os.remove(EMBEDDINGS_FILE)
                if os.path.exists(FAISS_INDEX_FILE):
                    os.remove(FAISS_INDEX_FILE)
            except Exception:
                pass
            st.success("ƒê√£ xo√° cache local.")
            st.rerun()

    st.sidebar.divider()

    try:
        files = _list_dropbox_files()
    except Exception as e:
        st.sidebar.error("L·ªói li·ªát k√™ Dropbox: %s" % e)
        files = []

    if files:
        st.sidebar.subheader("üìÅ T√†i li·ªáu trong Dropbox")
        for f in files[:100]:
            is_new = (f.get("id") or f.get("path")) not in processed_ids
            icon = "üÜï" if is_new else "‚úÖ"
            st.sidebar.caption("%s %s (%s)" % (icon, f["name"], format_file_size(f.get("size"))))

    logout_button()


def main():
    ok, username, display_name = login_gate()
    if not ok:
        st.stop()

    DROPBOX_FOLDER_PATH = st.secrets.get("DROPBOX_FOLDER_PATH") or os.getenv("DROPBOX_FOLDER_PATH", "/Apps/VNATechInsight")
    DROPBOX_FOLDER_URL = st.secrets.get("DROPBOX_FOLDER_URL", "")

    st.title("üõ©Ô∏è VNA TechInsight Hub (Dropbox Edition)")
    st.caption("H·ªá th·ªëng tra c·ª©u t√†i li·ªáu k·ªπ thu·∫≠t th√¥ng minh v·ªõi AI ‚Äì L∆∞u tr·ªØ tr√™n Dropbox")

    if DROPBOX_FOLDER_URL:
        st.link_button("üìÇ M·ªü th∆∞ m·ª•c Dropbox ƒë·ªÉ c·∫≠p nh·∫≠t ki·∫øn th·ª©c cho Chatbot", DROPBOX_FOLDER_URL)
    else:
        st.info(f"Th∆∞ m·ª•c Dropbox: `{DROPBOX_FOLDER_PATH}` (thi·∫øt l·∫≠p DROPBOX_FOLDER_URL trong secrets ƒë·ªÉ hi·ªán n√∫t m·ªü nhanh)")

    api_key = st.secrets.get("OPENAI_API_KEY")
    if not api_key:
        st.error("OPENAI_API_KEY is missing in secrets.")
        st.stop()
    client = OpenAI(api_key=api_key)

    force = st.session_state.get("force_rebuild", False)
    index, meta = _build_or_load_index(process_all=force)
    st.session_state["force_rebuild"] = False

    sidebar_panel(index, meta)

    st.subheader("üí¨ ƒê·∫∑t c√¢u h·ªèi")
    col1, col2 = st.columns([3, 1])
    with col1:
        question = st.text_input(
            "Nh·∫≠p c√¢u h·ªèi (ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh):",
            value="",
            placeholder="V√≠ d·ª•: Quy tr√¨nh ki·ªÉm tra ƒë·ªông c∆° CFM56 l√† g√¨?"
        )
    with col2:
        search_mode = st.selectbox(
            "Ch·∫ø ƒë·ªô t√¨m ki·∫øm",
            ["Hybrid (khuy·∫øn ngh·ªã)", "Semantic only", "Keyword priority"],
            index=0
        )

    with st.expander("‚öôÔ∏è T√πy ch·ªçn n√¢ng cao"):
        col_a, col_b = st.columns(2)
        with col_a:
            num_results = st.slider("S·ªë ngu·ªìn tham chi·∫øu", 5, 20, 10)
        with col_b:
            answer_detail = st.select_slider(
                "ƒê·ªô chi ti·∫øt c√¢u tr·∫£ l·ªùi",
                options=["Ng·∫Øn g·ªçn", "Trung b√¨nh", "Chi ti·∫øt"],
                value="Trung b√¨nh"
            )

    run = st.button("üîç T√¨m ki·∫øm & Tr·∫£ l·ªùi", type="primary", use_container_width=True)

    if run:
        if not question.strip():
            st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
            st.stop()
        with st.spinner("ƒêang ph√¢n t√≠ch c√¢u h·ªèi v√† t√¨m ki·∫øm t√†i li·ªáu..."):
            qvec = _embed_query(client, question)
            results = _search(index, meta, qvec, question, topk=num_results)
        if not results:
            st.info("‚ùå Kh√¥ng t√¨m th·∫•y ƒëo·∫°n tr√≠ch ph√π h·ª£p. Vui l√≤ng th·ª≠ c√¢u h·ªèi kh√°c ho·∫∑c ki·ªÉm tra t√†i li·ªáu.")
            return
        with st.spinner("ƒêang t·ªïng h·ª£p v√† ph√¢n t√≠ch th√¥ng tin..."):
            answer = _ask_llm(client, question, results)
        st.markdown("### ‚úÖ K·∫øt qu·∫£")
        st.markdown(answer)
        st.markdown("---")
        st.markdown("### üìö Ngu·ªìn tham chi·∫øu")
        df = pd.DataFrame([
            {
                "S·ªë": f"[{i+1}]",
                "T√™n file": r["file_name"],
                "Section": "%s %s" % (r.get("section_type","?"), r.get("section_number","?")),
                "Title": r.get("section_title", "")[:40],
                "Type": r.get("content_type", "general"),
                "Relevance": f"{r.get('rerank_score', 0):.3f}",
                "Semantic": f"{r['similarity']:.3f}",
                "Keyword": f"{r.get('keyword_score', 0):.3f}",
            }
            for i, r in enumerate(results)
        ])
        st.dataframe(df, use_container_width=True, hide_index=True)
        with st.expander("üìÑ Xem chi ti·∫øt c√°c ƒëo·∫°n tr√≠ch"):
            for i, c in enumerate(results, start=1):
                title = c.get("section_title", "")
                title_display = f" - {title}" if title else ""
                st.markdown(f"**[{i}] {c['file_name']}** ‚Äì {str(c.get('section_type','?')).title()} {c.get('section_number','?')}{title_display}")
                badges = []
                if c.get("content_type"): badges.append(f"üè∑Ô∏è {c['content_type']}")
                if c.get("has_tables"): badges.append("üìä Has tables")
                if c.get("has_lists"): badges.append("üìã Has lists")
                badges.append(f"‚≠ê {c.get('rerank_score', 0):.3f}")
                st.caption(" | ".join(badges))
                txt = c["text"]
                if len(txt) > 1500:
                    txt = txt[:1500] + "\n\n... (truncated)"
                st.code(txt, language="markdown")
                st.markdown('---')

    st.caption("S·∫£n ph·∫©m th·ª≠ nghi·ªám c·ªßa Ban K·ªπ thu·∫≠t ‚Äì VNA. M·ªçi √Ω ki·∫øn ƒë√≥ng g√≥p vui l√≤ng li√™n h·ªá Ph√≤ng K·ªπ thu·∫≠t M√°y bay.")

if __name__ == "__main__":
    main()
